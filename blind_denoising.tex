\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2021} with \usepackage[nohyperref]{icml2021} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2021}

% custom packages
\usepackage{amsmath}
\usepackage{amssymb}
% \usepackage{times}
% \usepackage{epsfig}
% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2021}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2021}

\begin{document}

\twocolumn[
\icmltitle{Joint self-supervised blind denoising and noise estimation}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2021
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Jean Ollion}{sabilab}
\icmlauthor{Charles Ollion}{cmap}
\icmlauthor{Sylvain Le Corff}{tsp,cmap}
\end{icmlauthorlist}

\icmlaffiliation{sabilab}{SABILab, Die, France}
\icmlaffiliation{cmap}{CMAP, Ecole Polytechnique, Institut Polytechnique de Paris, France}
\icmlaffiliation{tsp}{Samovar, T\'el\'ecom SudParis, D\'epartement CITI, Institut Polyechnique de Paris, France}

\icmlcorrespondingauthor{Cieua Vvvvv}{c.vvvvv@googol.com}
\icmlcorrespondingauthor{Eee Pppp}{ep@eden.co.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Blind Denoising, Self-supervised, Bio-image, Machine Learning}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.


%%%%%%%%% ABSTRACT
\begin{abstract}
Self-supervised deep neural networks trained for blind denoising have recently emerged and outperformed supervised networks. Assuming that the signal has local correlation and that the noise components are independent, such networks can be estimated without clean training data. Therefore, they are particularly relevant for biomedical image denoising where the noise is difficult to model precisely and clean training data are usually unavailable.
In this work, we introduce a model and  an implementation based on two neural networks trained simultaneously to jointly predict the clean signal and infer the noise distribution.% with few assumptions on the noise. %We propose an implementation based on two neural networks trained simultaneously.%, in which motivated architecture choices enable to significantly improve the quality of denoising as well as providing an accurate noise distribution.

Our method significantly outperforms current state-of-the-art self-supervised blind denoising algorithms, on six publicly available biomedical image datasets. We also show empirically with synthetic noisy data that our model captures the noise distribution efficiently. We also introduce a simple metric to estimate the sharpness of denoised images, which we hope will be used to better assess blind denoising quality. Finally, the described framework is simple, lightweight and computationally efficient, making it useful in practical cases.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

Image denoising is the process of recovering a clean signal $X$ given an observation $Y$ corrupted by a noise $\varepsilon$. Classical denoising approaches  are model-driven in the sense that they rely on assumptions on the noise distribution or on the structure of the signal but are often  limited by the relevance of these assumptions.
Recently, efficient data-driven methods have emerged. Most of them assume that pairs made of noisy data $Y$ associated with a clean signal $X$ are available in a supervised learning framework, see for instance \cite{weigert2017content}. In \cite{lehtinen2018noise2noise}, the authors have demonstrated that it is possible to train an efficient denoising method using only pairs of independent noisy measurements $(Y^1, Y^2)$ of the same signal. Such assumptions have also been used to solve deconvolution problem with repeated measurements as in \cite{delaigle2008deconvolution}. However, obtaining  independent observations of the same signal is often unrealistic in practice.

Recent self-supervised methods have overcome this limitation \cite{batson2019noise2self,krull2018noise2void} by training a neural network to predict the value of a (corrupted) pixel value $Y$ only using the noisy observations of the surrounding pixels. The trained network has extracted some local structure in the signal and therefore can be used as a denoiser. Such approaches are referred to as \textit{blind-denoising} methods because they require few assumptions on the noise. This noise is assumed to be centered and the noise components associated with different observations are independent. This is well suited to typical microscopy settings, in which both the clean image is unavailable, and the real noise process is complex and not known.

Current denoising neural networks make use of convolutional layers, which have a receptive field, centered on a pixel $Y$. Through self-supervision, the network predicts $Y$ given the noisy observations of the surrounding pixels. It is pivotal that the network does not use $Y$ as an input as the learning task would boil down to recovering one of the input data. A common approach is then to ensure that $Y$ is not used in the computation, either by masking it or by modifying the convolutional architecture.

In this work, we propose a method which is able to recover both the original signal and the noise distribution. Our only assumption is that the noise components associated with the different observations are independent and centered. It introduces a simple mathematical framework, which we hope will lead to a better understanding on the inference procedure. Practically, our framework is simple, fast, flexible as it is based on standard neural network layers, and produces significantly better metrics over 6 standard microscopy datasets. %The code will be  released after the reviewing process.

\section{Related work}
\label{sec:related}
\paragraph{Denoising by masking observed data.}
Self-supervised denoising methods were introduced by Noise2Void (N2V) \cite{krull2018noise2void} and Noise2Self (N2S) \cite{batson2019noise2self}. They rely on training a function $f_\theta$ depending on an unknown parameter $\theta$, usually  implemented as a convolutional neural network such as  a U-net, through masked self-supervision, minimizing the following objective:

$$\mathcal{L}: \theta\mapsto \frac{1}{M}\sum_{i=1}^M \|f_\theta(Y^{masked}_i) - Y_i\|^2\,, $$
where $Y^{masked}_i$ is the image in which the pixel $Y_i$  on which the loss is computed  have been masked. This masking step is crucial to foster learning of  local structure in the signal to predict the masked values.

In practice, the masking procedure can have a large impact on training, as only masked pixels are used for training (typically representing a few percent of the image). In N2V, $\{(Y^{masked}_i,Y_i)\}_{1\leqslant i\leqslant M}$ are sampled  randomly in the picture, and masking consists in replacing $Y_i$ by a random observed value in its neighborhood (with a positive probability of replacing $Y_i$ by itself meaning that no masking is introduced). In N2S,  $\{(Y^{masked}_i,Y_i)\}_{1\leqslant i\leqslant M}$ are obtained with  a fixed grid, and $Y_i$ is replaced by the average of neighboring observations. Even though the prediction of a noisy pixel conditionnally on its neighbors is an impossible task to learn perfectly, the estimated function $f_{\hat \theta}$ shows strong denoising properties. 

These methods however suffer from high frequency denoising artifacts known as \textit{checkerboard pattern}. Finally, \cite{broaddus2020removing} showed that when the noise has  local correlations (for instance a  directional noise), the masking can be adapted (by masking adjacent pixel in the same direction as the noise spatial correlation for instance) to remove them. These works have been extended in DecoNoising \cite{goncharova2020} in which a Gaussian conlvolution is added after the neural network output to simulate microscope Point Spread Function. This technique improves performances, however the deconvolved image (predicted image before the Gaussian convolution) has even stronger checkerboard pattern.

\paragraph{J-invariant networks.}
Learning to predict a collection of held-out pixels of an image, from all the other pixels and masked versions (as in N2V, N2S) can fit in the J-invariant framework introduced by \cite{batson2019noise2self}.
A J-invariant function does not depend on a few selected dimensions J of its input variables; typically a few masked pixels
in the picture.
This usually translates in this function being a convolutional function which does not depend on the central pixel, but rather on all the observations in its neighborhood. Those functions are sometimes also called \textit{blind spot}.
% This independance guarantees that such a function learnt through self-supervision is not the identity.
 Additionally, \cite{batson2019noise2self} also showed theoretical guarantees that a J-invariant function may seperate noise from signal and produce near-optimal denoisers.

However, in practice, Neither N2V nor N2S use strictly J-invariant functions, as they remove the masks at inference time,
effectively using all the pixels, resulting in significantly better performance. By having this difference between train and test time, it is not well understood how the denoiser uses the central pixel information, as it was not seen during training. The performance boost might be due to the inductive bias of the convolutional architecture, but understanding further this dependency could lead to better performances.
Few other works have true J-invariant functions which are kept J-invariant at test time, however this is not trivially achieved in convolutional neural networks. This was achieved in \cite{laine2019high} by introducing directional convolution kernels, which only depend on pixels in a specific directions, ensuring that the value does not depend on pixels on the opposite direction. The function is therefore J-invariant, where J is a partition of the image, excluding the central pixel, depending on the direction chosen. One drawback is that the inference has to be performed four times, in each direction.
More recently \cite{lee2020noise2kernel} introduces a combination of specific convolutions operators with dilation and strides, ensuring that the function is independent of the central pixel by design, therefore J-invariant with J being the center pixel. This also benefits the training procedure, as they do not require a tuned masking procedure. However, this constrains strongly the network architecture, and the reported results are less competitive than other methods.

\paragraph{Benefits of a noise model.}
Among the denoising community, a few works have explicitely modeled the noise distribution. In \cite{laine2019high}, the authors used Gaussian and Poissonian noises. They either fix the noise parameters, or estimate them with an auxiliary neural network. As they model the signal distribution and the noise distribution, they can include the central pixel explicitely at test time in order to improve performances. This is only possible because they have a truely J-invariant function at test time. Their approach is consistant and yields good performance, but is then restricted to known, synthetic noise and therefore falls under the category of \textit{non-blind denoising} methods.
Intuitively, the test-time use of central pixel is also happening in other methods such as N2V or N2S, but the dependency on central pixel is not explicit and unknown.

Both methods can improve performances but still display a checkerboard pattern and sometimes display numeric instabilities during training \cite{2020DivNoising}.

\textit{charles: jean peux tu m'aider sur cette partie, je pense que c'est important de bien expliquer et je veux pas dire de bêtises, tu connais mieux ces travaux que moi}
\cite{krull2019probabilistic,prakash2020fully,2020DivNoising} make use of a noise model, which is a more generic modelisation of noise depending on the signal intensity: $p(y|x) = N(x): \mathbb{R} \to \mathbb{R}$. In practice, a noise function (or network)  models a noise which depends on the intensity of the signal, which can cover much more realistic noises, combination of gaussian, speckle, poisson, etc. This noise then better model approximates the real world noise. An interesting feature of these works is the possibility to introspect the noise model after training.
Probabilistic N2V \cite{krull2019probabilistic} requires additional data to calibrate, and therefore is no longer a fully self-supervised blind denoising method. Moreover the network is trained to predict a whole histogram distribution of possible noisy values, resulting approximating the noise distribution at each pixel. However, this is inefficient as it requires to predict $800$ values instead of a single point estimate of each pixel. On the other hand, \cite{prakash2020fully} builds implements a carefully designed gaussian mixture model to predict the noise level depending on the signal, but requires a calibration or bootstraping phase to learn it. The definition of the noise network is not trivial and can lead to numerical instabilities during training.
We build upon these works by proposing a fully jointly trainable and stable denoising network and noise network.
Finally, \cite{2020DivNoising} uses the Variational Autoencoder formalism, adding a pre-calibrated noise model in their architecture.

It is worth noting that supervised \textit{blind denoising} methods have used parametrized noise model, such as \cite{zhang2017beyond,yue2019variational}, which explicitely uses large neural networks to model a complex noise, with even less assumption on the noise (noise can be slightly structured, not centered...). Even though \cite{yue2019variational} is able to train jointly a noise network and a denoiser, their modeling only works in a supervised setting, which it does not apply in our setting.

Instead of focusing on finding new strictly J-invariant functions, our approch rather focuses on having efficient masking procedure only at train time, which results in better empirical performances\textit{[Jean] en est-on sûr ?}, and the ability to tune the masking to match structured noise (see section X)(\cite{broaddus2020removing} \textit{[Jean] suggestion: séparer en 2 phrases pour bien introduire l'avantage de prendre en compte des bruits structurés et insiter sur le fait qu'on a pas cette souplesse dans les vrai J-invariant networks}) . We provide a few directions to explain why relaxing the J-invariance might be beneficial.

\section{Model}
\label{sec:model}

Estimating a signal corrupted by additive noise  is a challenging statistical problem. In such frameworks, the received observation $Y$ is given by $Y = X + \varepsilon$,  where $X$ is the signal and $\varepsilon$ is the noise. A lot of works have been devoted to deconvolution where the aim is to recover the distribution of the signal based on the observations. It has been applied in a large variety of disciplines and has stimulated a great research interest in signal processing \cite{moulines1997maximum,attias1998blind}, in image reconstruction \cite{kundur1996blind,campisi2017blind}, see also  \cite{meister:2009}. 


Recently, \cite{gassiat:lecorff:lehericy:2021} proved that it is possible to recover the signal distribution when $X$ has at least two dimensions and may be decomposed into two subsets of random variables which satisfy some weak dependency assumption. This identifiability result does not require any assumption on the noise distribution but illustrates that the components of the signal must be dependent to allow its identification. The results proposed in \cite{gassiat:lecorff:lehericy:2021} motivate the denoising approach introduced in this paper where several classes of noises are considered to describe the observations received from the target signal.  %The signal $X$ is modeled as a function of the noisy observation $Y$ and its neighborhood $\Omega_Y$: $X =  \mu_{\theta_p}(\Omega_Y;Y)$.

Let $X$ be the target signal in a pixel and $\Omega_X$ be the signal values in a neighborhood of $X$.  The observation $Y$ associated with $X$  is given by
\begin{equation}
\label{eq:def:Y}
Y = X + \varepsilon\,,
\end{equation}
%which means that
%$$
%Y = X + (\mu_{\theta_p}(\Omega_Y;\overline{\Omega_Y})-\mu_{\theta_p}(\Omega_Y;Y)) + \sigma_{\theta_n}(\mu_{\theta_p}(\Omega_Y;\overline{\Omega_Y}))\varepsilon
%$$
%The blind denoising framework introduced in this paper can be written as a regression problem where the observations conditionally on $\Omega_Y$ the observation is given by
%$$
%Y = \mu_{\theta_p}(\Omega_Y;\overline{\Omega_Y}) + \sigma_{\theta_n}(\mu_{\theta_p}(\Omega_Y;\overline{\Omega_Y}))\varepsilon\,,
%$$
where $\varepsilon$ is centered noise which depends on $(X,\Omega_X)$. The assumption that the noise is state-dependent is crucial in the experiments as detailed in Section~\ref{}. In standard approaches \cite{},  $\varepsilon$ is assumed to be a centered Gaussian random variable with known variance $\sigma^2$ \cite{} or $\varepsilon$ has a Poisson-Gaussian distribution, i.e. it  is a centered Gaussian random variable with variance $\alpha X + \eta^2$ \cite{}.
Conditionally to $X$, we propose a more general corrupting process as $\varepsilon$ is a centered Gaussian random variable with variance $\sigma^2_{\theta_n}(X)$ where $\sigma^2_{\theta_n}$ is parameterized by a convolutional neural network, see Section~\ref{}. We also propose an extension of this setting in which $\varepsilon$ is a mixture of Gaussian random variables with state-dependent variances to account for skewness.

In this paper, we assume that $(X,\Omega_X)$ is a random vector with dependent variables and propose to model the conditional mean of $X$ given $(Y,\Omega_Y)$ by a parametric function denoted by $\mu_{\theta_p}$ so that $\mathbb{E}[X|Y,\Omega_Y] = \mu_{\theta_p}(\Omega_Y,Y)$ where $\Omega_Y$ are the noisy observations of the signals in the neighborhood of $X$. Therefore, a natural estimator of $X$ given the noisy observations is given by $\widehat X = \mu_{\theta_p}(\Omega_Y,Y)$.

In this paper, $\mu_{\theta_p}$ is modeled as a convolutional neural network, see Section~\ref{} for more details, and during training this predictor $\widehat X$ cannot be used to estimate $\theta_p$ and $\theta_n$ as  a standard mean squared error loss would recover the prediction of $Y$ instead of $X$. This is the reason why we adopt a blind spot denoising approach and assume during training that $\mu_{\theta_p}$ cannot use $Y$ as an input which must be replaced by an estimator. In this framework, an appealing prediction of $Y$ is given by $\mathbb{E}[Y|\Omega_Y]$ which we estimate by $\mu_{\theta_p}(\Omega_Y,g(\Omega_Y))$ where $g$ (\textcolor{red}{Autre chose que la moyenne empirique ?}).

%The signal $X$ is then modeled as
%\begin{equation}
%\label{eq:def:X}
%X =  \mu_{\theta_p}(\Omega_X,U)\,,
%\end{equation}
%where $\Omega_X$ are the signal values in a neighborhood of $X$ and $U$ is a random variable on $\mathbb{R}$. % $\Omega_X$-measurable.
% In the case where the random variable $U$ is $\Omega_X$-measurable, there exists a measurable function $g$ such that $U = g(\Omega_X)$.
Combining this with the additive model  \eqref{eq:def:Y} yields the following loss function associated with $N$ observations $(Y_1,\ldots,Y_N)$:
$$
\ell_{\theta}: (Y_1,\ldots,Y_N) \mapsto \frac{1}{N}\sum_{i=1}^N \ell_{\theta}(Y_i|\Omega_{Y_i})\,,
$$
where $\theta = (\theta_n,\theta_p)$ and
\begin{multline*}
\ell_{\theta}(Y_i|\Omega_{Y_i}) = \frac{1}{2}\log(\sigma_{\theta_n}( \mu_{\theta_p}(\Omega_{Y_i},g(\Omega_{Y_i})))^2) \\
+\frac{1}{2}\left(\frac{Y_i-\mu_{\theta_p}(\Omega_{Y_i},g(\Omega_{Y_i}))}{\sigma_{\theta_n}(\mu_{\theta_p}(\Omega_{Y_i},g(\Omega_{Y_i}))}\right)^2\,. %\frac{(Y_i-\mu_{\theta_p}(\Omega_{Y_i};\overline{\Omega_{Y_i}}))^2}{\sigma_{\theta_n}(\mu_{\theta_p}(\Omega_{Y_i};\overline{\Omega_{Y_i}}))^2} \,,
\end{multline*}
%This loss is obtained by computing the conditional loglikelihood of $Y$ given $X$ evaluated at the predictor of $Y$ obtained by replacing $\Omega_{X}$ by the observed values $\Omega(Y)$ in \eqref{eq:def:X}.
In Section~\ref{sec:experiments} we propose several distributions $\varphi$ to account for positive skewness which cannot be modeled with Gaussian distributions. We also provide empirical evaluations that choosing $g$ as the empirical mean of its entries is a robust and appealing solution while other choices can be made straightforwardly.
%The random variable $U$ being  $\Omega_X$-measurable, there exists a measurable function $g$ such that $U = g(\Omega_X)$

%After a training phase to estimate $\theta$, the signal values cannot be predicted as the random variable $U$ is not observed. Several predictors of $X$ can be designed.



%uses the fact that by \eqref{eq:def:X} $Y_i$ is a natural linear predictor of $X_i$ and then
%instead of introducing an explicit noise  distribution in the definition of $U$ and  therefore in \eqref{eq:def:X} and then to estimate the posterior distribution of $X$ given $Y$ we propose to introduce a loss function which

% and $\overline{\Omega_Y}$ is the empirical mean of $\Omega_Y$.
%The approach proposed in this paper is decomposed into two steps.
%\begin{enumerate}
%\item First, $\mu_{\theta_p}$ is obtained with the P-Net fed with $\Omega_y$ and $\overline \Omega_y$ at central position, $\sigma_{\theta_n}^2$ is  obtained with the N-Net fed with $\mu_{\theta_p}$.
%\item After a training procedure to estimate the parameters of these networks, the estimator of the unknow pixel given the observations $(y,\Omega_y)$ is set to $\mu_{\theta_p}(\Omega_y,y)$ as an approximation of the posterior mean.
%\end{enumerate}
%The signal $X$ is then modeled as $X =  \mu_{\theta_p}(\Omega_Y;Y)$ so that
%$$
%Y = X + \Delta_{\theta_p}(\Omega_Y;\overline{\Omega_Y},Y) + \sigma_{\theta_n}(\mu_{\theta_p}(\Omega_Y;\overline{\Omega_Y}))\varepsilon\,,
%$$
%where
%\begin{align*}
%\Delta_{\theta_p}(\Omega_Y;\overline{\Omega_Y},Y)  &=  \mu_{\theta_p}(\Omega_Y;\overline{\Omega_Y})  -\mu_{\theta_p}(\Omega_Y;Y) \,,\\
%&\sim \partial_2\mu_{\theta_p}(\Omega_Y;Y)[\overline{\Omega_Y}-Y]
%\end{align*}


\section{Experiments}
\subsection{Model Architecture}
\paragraph{P-Net}
The function $x\mapsto \mu_p$ is implemented through a UNet \cite{ronneberger2015u} deep neural network, which has a fully convolutional architecture.
We made a few changes from the original version: we do not crop the image and use zero-padding instead, we use 2 levels of contractions/expansions with 64 filters, expansions is performed by an upsampling layer with nearest-neighbor approximation directly followed by 2x2 convolution, we added two layers of 1x1 convolution with 64 filters and ReLU activation at the end of the network, and set no activation function at the output layer.
The main difference with the networks used in N2S and N2N is that we use upsampling layer instead of transpose convolution, as we observed transpose convolution tend to increase the checkerboard artefact.
The receptive field of this network is 35x35 pixels.
For natural images, we used 3 levels of contractions/expansions and 128 filters.

\textit{[Jean] cette section ne devrait-elle pas aller ailleurs ? ou supprimée?}
This implies that at a given coordinate $(l,m)$, $(\mu_p^{(l,m)}, \sigma_p^{(l,m)})$ depend on $x^{(l,m)}$ and its neighborhood.
While the central pixel $x^{(l,m)}$ is masked during training (its value is replaced with a deterministic function of the neighboring pixels), the convolutional architecture still uses the replaced value and learns the parameters of the convolution associated with this central value.

\paragraph{N-Net}
The function describing the noise distribution is implemented through a fully convolutional network composed of six 1x1 convolutions layers of 64 filters, each followed by a non-linear activation layer (alternatively tanh and relu).
We found that such networks could be trained in a supervised way with a clean image x as input and the following cost function, where y is a corrupted version of x:
$$
y\mapsto\varphi_{0,\sigma_n^2}(x)
$$
The essential aspect of the architecture is that the network contain no spatial convolution (only 1x1 convolutions), otherwise the noise distribution is not well described by the network. This is consistent with our model, in which the noise is independent of the neighborhood.

\subsection{Self-supervised loss}
Following \cite{batson2019noise2self}, we used a self-supervised loss with pixel masking along a grid. The loss is computed only on the masked pixels.
We obtained the best results with a replacement by a 3x3 gaussian filter with $\sigma=1$ with a weight equal to zero at the center position.
The drawback of maksing along a grid is that for a given receptive field, pixels are masked at fixed positions. If grid spacing is too low, then too many masked pixels are present in the receptive field and perturbs the performances, because the network learn to avoid using masked pixels for prediction. On the other hand, the larger the spacing, the less pixels are used for training, which reduces dramatically training efficiency.
In order to push the limits of this trade-off between learning efficiency and denoising quality, we introduced two modifications on the original approach: we used a random dynamic spacing between 3 and 5 pixels and we also dropped out randomly 10\% of the grid points. Both modifications allow to have relative positions of masked pixels that change randomly, and thus reduce considerably the grid spacing without drop of performances. The value of the loss was normalized by the number of points in the grid.

Fluorescence-microscopy datasets usually display a high signal frequency imbalance, where background is overrepresented. To cope with this issue, we introduced a weight map to give more relative importance to rare signal during training. Let $p(x)$ the observed probability of a value within the training dataset and G the masking grid for a given mini-batch, the associated weight is $\frac{1 - p(x)}{\sum_{G}(1 - p(x))}$

\subsection{Datasets}
We trained and evaluated our method on 6 publicly available datasets of microscopy images. In those datasets, ground truth is estimated by averaging several observations of the same field-of-view (FOV).

The 3 first datasets have been published along with the PN2V method\cite{krull2019probabilistic}, each is composed of several observations of a single FOV. \emph{Convallaria} dataset is refered to as \emph{PN2V-C}, \emph{Mouse skull nuclei} as \emph{PN2V-MN} and \emph{Mouse Actin} as \emph{PN2V-MA}. We used the same training and evaluation sets as the authors: for each sample type the whole dataset is used for training, and only a subset of the FOV is used for PSNR computation: $(Y,X)\in([0, 512], [0, 512])$ for PN2V-C, $([0, 512], [0, 256])$ for PN2V-MN and $([0, 1024], [0, 512])$ for PN2V-MA.

The 3 last datasets are the 3 channels of the W2S dataset\cite{zhou2020w2s} refered to as \emph{W2S-1}, \emph{W2S-2} and \emph{W2S-3}. We used the 16-bit raw images kindly provided by the authors. The dataset is composed of 120 FOV, the first 80 are used for training and the last 40 for evaluation. Following the authors, for each FOV, only the raw image of index 249 is used for training and for evaluation.

This dataset contains also alinged super-resolution images (referred to as SIM-GT) which were used to evaluate the deconvolution procedure. We downscaled the SIM-GT image using a 2x2 average pooling transform in order to have a pixel-wise correspondance with wide-field (WF) images. In order to have the SIM-GT and WF images in the same intensity range, for each FOV we scaled linearily the SIM-GT image in order to have the modal values and the 95\% percentile correspond with the WF ground truth image.

\subsection{Training}
Networks are trained using Adam optimizer with a learning rate of $10^{-4}$, decreased by $\frac{1}{2}$ on plateau of 15 epochs until $10^{-6}$. We trained networks for 300 steps of 100 epochs.
We obtained better and more reproductible results using the weights of the trained model at the last epoch instead of the weights of the model with the best validation loss, possibliy because the loss is a bad proxy for the denoising performances. For that reason, we did not use a validation step.
Each batch is composed of (overlapping) random tiles of 96x96 pixels for microscopy datasets. Batch size was set to 4 and tile overlap to a value that yielded in mini batches of 64 tiles. Tiles are augmented with random horizontal and/or vertical flip and/or a random rotation with an angle chosen within $(90, 180, 270)$.
[For natural images tile size was set to 128x128.]
\begin{itemize}
\item average flip
\item alternate procedure ?
\end{itemize}

\subsection{Evaluation}
For the 6 chosen datasets, images are encoded in 16-bit thus the actual data range of each ground truth image is used for PSNR computation.
When comparing images, the PSNR is not highly indicative of perceived similarity, in particular it do not reflect similarity of high frequency information such as textures and local contrasts\cite{wang2004image}, that denoising methods tend to reduce. It is thus essential to have other metrics that them into account.
To address this shortcoming, we used Structural similarity (SSIM) that take texture into account. It was computed as in \cite{wang2004image}.
In order to take into account local contrasts, we introduced a very simple metric: we computed PSNR of gradient magnitude images (GPSNR). This metric simply reflects whether an image is blurry or sharp, compared to a ground truth image.
GPSNR is computed by transforming both ground truth and sample images with gaussian gradient magnitude with $\sigma=1$, using the range of the gradient magnitude transform of the ground truth image.

\section{Results}
We compared our method to 2 baselines the current state-of-the-art blind self-supervised denoising method for bio-images, DecoNoising, as well as one of the most simple denoising method: convolution by a gaussian. For each dataset we chose the optimal standard deviation of the gaussian so that it maximizes the PSNR. We believe it makes a good reference, as it actually removed efficiently noise, but also local contrasts, so a good denoising method should perform better both in terms of PNSR and GPSNR/SSIM. We were suprised to notice that N2V do not permforms better than the optimal gaussian in terms of PSNR for 3 of the 6 datasets.


\section{Conclusion}



{\small
\bibliography{blind_denoising}
\bibliographystyle{icml2021}
}

\end{document}
