\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2021} with \usepackage[nohyperref]{icml2021} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2021}

% custom packages
\usepackage{amsmath}
\usepackage{amssymb}
% \usepackage{times}
% \usepackage{epsfig}
% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2021}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2021}

\begin{document}

\twocolumn[
\icmltitle{Joint self-supervised blind denoising and noise estimation}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2021
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Jean Ollion}{sabilab}
\icmlauthor{Charles Ollion}{cmap}
\icmlauthor{Sylvain Le Corff}{cmap}
\end{icmlauthorlist}

\icmlaffiliation{sabilab}{SABILab, Die, France}
\icmlaffiliation{cmap}{CMAP, Ecole Polytechnique, Universit√© Paris-Saclay, France}


\icmlcorrespondingauthor{Cieua Vvvvv}{c.vvvvv@googol.com}
\icmlcorrespondingauthor{Eee Pppp}{ep@eden.co.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Blind Denoising, Self-supervised, Bio-image, Machine Learning}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.


%%%%%%%%% ABSTRACT
\begin{abstract}
Self-supervised deep neural networks trained for blind denoising have recently emerged and outperformed supervised networks. Using the assumption that the signal has local correlation and that the noise components are independent, such networks are able to predict an estimate of the clean signal without clean training data. Therefore they are particularly relevant for biomedical image denoising where the noise process is difficult to model precisely and clean training data are usually unavailable.
In this work we describe a model that estimates the clean signal and the noise distribution jointly with very few assumptions on the noise. This model is implemented through two neural networks trained jointly, in which motivated architecture choices enable to improve the quality of denoising as well as providing an accurate noise distribution.

Our method improves significantly the performance of current state-of-the-art self-supervised blind denoising, on six publicly available biomedical image datasets. We also show empirically on synthetic noisy data that we are able to capture the noise distribution efficiently. We also introduce a simple metric to estimate the sharpness of denoised images, which we hope will be used by the community to better assess blind denoising quality. Finally, the described framework is simple, lightweight and computationally efficient, making it useful in practical cases.
\end{abstract}

\section{Introduction}
\label{sec:introduction}
Classical denoising methods are model-driven methods in the sense that they rely on assumptions on the noise process or on the structure of the signal but are limited by the relevance of these assumptions.
Recently, efficient data-driven methods have emerged. Most of them are using pairs of noisy and clean measurements of the same signal in a supervised learning problem \cite{weigert2017content}. Lethinen et al. have demonstrated that it is possible to train an efficient denoising method using pairs of independent noisy measurements instead of a clean measurement of the same signal \cite{lehtinen2018noise2noise}. However clean measurements or pairs of independent noisy measurements of the siganl are often unavailable in practice.
Recent self-supervised methods overcome this limitation \cite{batson2019noise2self,krull2018noise2void}. The main idea of those methods is that a neural network learns to predict a clean pixel without having access to the value of this pixel but only using its surroundings. We refer to those methods as blind-denoising methods because they make no assumption on the nature of the corrupting process except that it is centered and that its components are independent.
The previously cited methods as well as their improved versions all display a high-frequency artefact refered to as \textit{checkerboard pattern} that limit their denoising capabilities.

In this work we improve the denoising performance of blind denoising networks through the introduction of an auxiliary network that is able to capture the distribution of the noise.

\section{Related work}
\label{sec:related}

Two types of self-supervised methods:
\paragraph{Masked training}
\begin{itemize}
\item Introduced by Noise2Noise\cite{krull2018noise2void} (N2N) and Noise2Self \cite{batson2019noise2self} (N2S).
\item describe self-supervised loss. only masked pixels are used for training, which represent a few percent per image.
\item masking strategies. explain the differences and flaws: N2N: no minimal distance between masked pixels + a chance that a pixel is replaced by itself/ N2S: grid replacement: mask also other pixels of the receptive field + replace by donut average.
\item PN2V\cite{krull2019probabilistic} and PPN2V\cite{prakash2020fully}: use a noise model, which slighlty improve performances without solving the checkerboard pattern.
\item deconoising:\cite{goncharova2020} add a gaussian conlolution after the NN output to simulate microscope PSF improves performances, however the deconvolved image (predicted image before the gaussian conlolution) has even stronger checkerboard pattern.
\item In a related work, Kobayashi et al. show that self-supervised netwoks can perform efficiel noise-tolerant deconvolution, on sythetic data \cite{kobayashi2020image}.
\item When noise processe displays local correlations that are known masking can be adapted to remove them \cite{broaddus2020removing}
\end{itemize}

\paragraph{J-invariant networks}
Learning to predict a collection of held-out pixels of an image, from all the other pixels and mased versions (\cite{krull2018noise2void}) can fit in the
J-invariant framework introduced by \cite{batson2019noise2self}. A J-invariant function $f$ does not depend on a few selected dimensions J ; typically a few masked pixels
in the picture. This usually translates in $f$ being a convolutional function which does not depend on the central pixel, but rather on the neighboring ones. This independance guarantees that the function $f$, learnt through self supervision $||f(x) - x||$ is not the identity.

Additionally, \cite{batson2019noise2self} also show theoretical guarantees that the J-invariant function may seperate noise from signal and produce near-optimal denoisers.
However in practice, Neither \cite{krull2018noise2void} nor \cite{batson2019noise2self} use strictly J-invariant functions, as they remove the masks at inference time,
effectively using all the pixels, resulting in significantly better performance. By having this difference between train and test time, it is not well understood what the unmasked function actually models.
Few other works have true J-invariant functions which are kept J-invariant at test time, however this is not trivially achieved in convolutional neural networks. \cite{laine2019high} achieves this by introducing directional convlution kernels, which only depend on pixels in a specific directions, ensuring that the value does not depend on pixels on the opposite direction. The function is therefore J-invariant, where J is a partition of the image, excluding the central pixel, depending on the direction chosen. One drawback is that the inference has to be performed four times, in each direction. More recently \cite{lee2020noise2kernel} introduces a combination of specific convolutions operators with dilation and strides, ensuring that the function is independant of the central pixel by design, therefore J-invariant with J being the center pixel. This also benefits the training procedure, as they do not require a tuned masking procedure. However this constrains strongly the network architecture, and the reported results are less competitive than other methods.

Instead of focusing on finding new strictly J-invariant functions, our approch rather focuses on having efficient masking procedure only at train time, which results in better empirical performances\textit{[Jean] en est-on s√ªr ?}, and the ability to tune the masking to match structured noise (see section X)(\cite{broaddus2020removing} \textit{[Jean] suggestion: s√©parer en 2 phrases pour bien introduire l'avantage de prendre en compte des bruits structur√©s et insiter sur le fait qu'on a pas cette souplesse dans les vrai J-invariant networks}) . We provide a few directions to explain why relaxing the J-invariance might be beneficial.

\textit{[Jean] Faire une section related work sur l'apport d'un model de bruit}
\paragraph{Benefits of a noise model}
\begin{itemize}
\item what do we call a noise model: p(y|x) scalar
\item \cite{laine2019high} : Introduction of a noise model $\rightarrow$ allows to improve performances on synthetic noise, but not suitable for real noise. Intuition that J-invariant functions can be improved by adding the information of the central pixel.
\item Two methods derived from N2V are using a noise noise model that approximate observed real noise to improve denoising performances: PN2V \cite{krull2019probabilistic} and PPN2V \cite{prakash2020fully}. The former require additional information on the noise and the latter uses a previously trained network to estimate the noise. The denoising networks are trained to predict 800 images with the previously fitted ditribution\textit{[Jean] mieux expliquer, et dire que c'est pas efficace de pr√©dire 800 channels}.
Both methods can improve performances but still display a checkerboard pattern and sometimes display numeric instabilities during training \cite{2020DivNoising}.

\end{itemize}

\section{Model}
\label{sec:model}

Estimating a signal corrupted by additive noise  is a challenging statistical problem. In such frameworks, the received observation $Y$ is given by
\begin{equation*}
Y = X + \varepsilon\,,
\end{equation*}
where $X$ is the signal and $\varepsilon$ is the noise. A lot of works have been devoted to deconvolution where the aim is to recover the distribution of the signal based on the observations. It has been applied in a large variety of disciplines and has stimulated a great research interest in signal processing \cite{moulines1997maximum,attias1998blind}, in image reconstruction \cite{kundur1996blind,campisi2017blind}, see also  \cite{meister:2009}.

Recently, \cite{gassiat:lecorff:lehericy:2021} proved that it is possible to recover the signal distribution when $X$ has at least two dimensions and may be decomposed into two subsets of random variables which satisfy some weak dependency assumption. This identifiability result does not require any assumption on the noise distribution but illustrates that the components of the signal must be dependent to allow its identification. The results proposed in \cite{gassiat:lecorff:lehericy:2021} motivate the denoising approach introduced in this paper where several classes of noises are considered to describe the observations received from the target signal.  %The signal $X$ is modeled as a function of the noisy observation $Y$ and its neighborhood $\Omega_Y$: $X =  \mu_{\theta_p}(\Omega_Y;Y)$.

Let $X$ be the target signal in a pixel and $\Omega_X$ be the signal values in a neighborhood of $X$.  The observation $Y$ associated with $X$  is given by
\begin{equation}
\label{eq:def:Y}
Y = X + \sigma_{\theta_n}(X)\varepsilon\,,
\end{equation}
%which means that
%$$
%Y = X + (\mu_{\theta_p}(\Omega_Y;\overline{\Omega_Y})-\mu_{\theta_p}(\Omega_Y;Y)) + \sigma_{\theta_n}(\mu_{\theta_p}(\Omega_Y;\overline{\Omega_Y}))\varepsilon
%$$
%The blind denoising framework introduced in this paper can be written as a regression problem where the observations conditionally on $\Omega_Y$ the observation is given by
%$$
%Y = \mu_{\theta_p}(\Omega_Y;\overline{\Omega_Y}) + \sigma_{\theta_n}(\mu_{\theta_p}(\Omega_Y;\overline{\Omega_Y}))\varepsilon\,,
%$$
where $\varepsilon$ is centered noise independent of $(X,\Omega_X)$ with probability density function $\varphi$ with respect to the Lebesgue measure. In this paper, we assume that $(X,\Omega_X)$ is a random vector with dependent variables and propose to model the conditional mean of $X$ given by a parametric function, called P-net and denoted by $\mu_{\theta_p}$: $\mathbb{E}[X|Y,\Omega_Y] = \mu_{\theta_p}(\Omega_Y,Y)$ where $\Omega_Y$ are the noisy observations of the signals in the neighborhood of $X$. Therefore, a natural estimator of $X$ given the noisy observations is given by $\widehat X = \mu_{\theta_p}(\Omega_Y,Y)$.

In this paper $\mu_{\theta_p}$ is modeled as a convolutional neural network, see Section~\ref{} for more details, and during training this predictor $\widehat X$ cannot be used to estimate $\theta_p$ and $\theta_n$ as  a standard mean squared error loss would recover the prediction of $Y$ instead of $X$ (\textcolor{red}{A reprendre plus precisement}). This is the reason why we adopt a blind spot denoising approach and assume during training that $\mu_{\theta_p}$ cannot use $Y$ as an input which must be replaced by an estimator. In this framework, we  consider that an appealing prediction of $Y$ is given by $\mathbb{E}[Y|\Omega_Y]$ which we estimate by $\mu_{\theta_p}(\Omega_Y,g(\Omega_Y))$ where $g$ ((\textcolor{red}{Autre chose que la moyenne empirique ?}).

%The signal $X$ is then modeled as
%\begin{equation}
%\label{eq:def:X}
%X =  \mu_{\theta_p}(\Omega_X,U)\,,
%\end{equation}
%where $\Omega_X$ are the signal values in a neighborhood of $X$ and $U$ is a random variable on $\mathbb{R}$. % $\Omega_X$-measurable.
% In the case where the random variable $U$ is $\Omega_X$-measurable, there exists a measurable function $g$ such that $U = g(\Omega_X)$.
Combining this with the additive model  \eqref{eq:def:Y} yields the following loss function associated with $N$observations $(Y_1,\ldots,Y_N)$:
$$
\ell_{\theta}: (Y_1,\ldots,Y_N) \mapsto \frac{1}{N}\sum_{i=1}^N \ell_{\theta}(Y_i|\Omega_{Y_i})\,,
$$
where $\theta = (\theta_n,\theta_p)$ and
\begin{multline*}
\ell_{\theta}(Y_i|\Omega_{Y_i}) = \frac{1}{2}\log(\sigma_{\theta_n}( \mu_{\theta_p}(\Omega_{Y_i},g(\Omega_{Y_i})))^2) \\
-\log \varphi\left(\frac{Y_i-\mu_{\theta_p}(\Omega_{Y_i},g(\Omega_{Y_i}))}{\sigma_{\theta_n}(\mu_{\theta_p}(\Omega_{Y_i},g(\Omega_{Y_i}))}\right)\,. %\frac{(Y_i-\mu_{\theta_p}(\Omega_{Y_i};\overline{\Omega_{Y_i}}))^2}{\sigma_{\theta_n}(\mu_{\theta_p}(\Omega_{Y_i};\overline{\Omega_{Y_i}}))^2} \,,
\end{multline*}
%This loss is obtained by computing the conditional loglikelihood of $Y$ given $X$ evaluated at the predictor of $Y$ obtained by replacing $\Omega_{X}$ by the observed values $\Omega(Y)$ in \eqref{eq:def:X}.
In Section~\ref{sec:experiments} we propose several distributions $\varphi$ to account for positive skewness which cannot be modeled with Gaussian distributions. We also provide empirical evaluations that choosing $g$ as the empirical mean of its entries is a robust and appealing solution while other choices can be made straightforwardly.
%The random variable $U$ being  $\Omega_X$-measurable, there exists a measurable function $g$ such that $U = g(\Omega_X)$

%After a training phase to estimate $\theta$, the signal values cannot be predicted as the random variable $U$ is not observed. Several predictors of $X$ can be designed.



%uses the fact that by \eqref{eq:def:X} $Y_i$ is a natural linear predictor of $X_i$ and then
%instead of introducing an explicit noise  distribution in the definition of $U$ and  therefore in \eqref{eq:def:X} and then to estimate the posterior distribution of $X$ given $Y$ we propose to introduce a loss function which

% and $\overline{\Omega_Y}$ is the empirical mean of $\Omega_Y$.
%The approach proposed in this paper is decomposed into two steps.
%\begin{enumerate}
%\item First, $\mu_{\theta_p}$ is obtained with the P-Net fed with $\Omega_y$ and $\overline \Omega_y$ at central position, $\sigma_{\theta_n}^2$ is  obtained with the N-Net fed with $\mu_{\theta_p}$.
%\item After a training procedure to estimate the parameters of these networks, the estimator of the unknow pixel given the observations $(y,\Omega_y)$ is set to $\mu_{\theta_p}(\Omega_y,y)$ as an approximation of the posterior mean.
%\end{enumerate}
%The signal $X$ is then modeled as $X =  \mu_{\theta_p}(\Omega_Y;Y)$ so that
%$$
%Y = X + \Delta_{\theta_p}(\Omega_Y;\overline{\Omega_Y},Y) + \sigma_{\theta_n}(\mu_{\theta_p}(\Omega_Y;\overline{\Omega_Y}))\varepsilon\,,
%$$
%where
%\begin{align*}
%\Delta_{\theta_p}(\Omega_Y;\overline{\Omega_Y},Y)  &=  \mu_{\theta_p}(\Omega_Y;\overline{\Omega_Y})  -\mu_{\theta_p}(\Omega_Y;Y) \,,\\
%&\sim \partial_2\mu_{\theta_p}(\Omega_Y;Y)[\overline{\Omega_Y}-Y]
%\end{align*}


\section{Experiments}
\subsection{Model Architecture}
\paragraph{P-Net}
The function $x\mapsto \mu_p$ is implemented through a UNet \cite{ronneberger2015u} deep neural network, which has a fully convolutional architecture.
We made a few changes from the original version: we do not crop the image and use zero-padding instead, we use 2 levels of contractions/expansions with 64 filters, expansions is performed by an upsampling layer with nearest-neighbor approximation directly followed by 2x2 convolution, we added two layers of 1x1 convolution with 64 filters and ReLU activation at the end of the network, and set no activation function at the output layer.
The main difference with the networks used in N2S and N2N is that we use upsampling layer instead of transpose convolution, as we observed transpose convolution tend to increase the checkerboard artefact.
The receptive field of this network is 35x35 pixels.
For natural images, we used 3 levels of contractions/expansions and 128 filters.

\textit{[Jean] cette section ne devrait-elle pas aller ailleurs ? ou supprim√©e?}
This implies that at a given coordinate $(l,m)$, $(\mu_p^{(l,m)}, \sigma_p^{(l,m)})$ depend on $x^{(l,m)}$ and its neighborhood.
While the central pixel $x^{(l,m)}$ is masked during training (its value is replaced with a deterministic function of the neighboring pixels), the convolutional architecture still uses the replaced value and learns the parameters of the convolution associated with this central value.

\paragraph{N-Net}
The function describing the noise distribution is implemented through a fully convolutional network composed of six 1x1 convolutions layers of 64 filters, each followed by a non-linear activation layer (alternatively tanh and relu).
We found that such networks could be trained in a supervised way with a clean image x as input and the following cost function, where y is a corrupted version of x:
$$
y\mapsto\varphi_{0,\sigma_n^2}(x)
$$
The essential aspect of the architecture is that the network contain no spatial convolution (only 1x1 convolutions), otherwise the noise distribution is not well described by the network. This is consistent with our model, in which the noise is independent of the neighborhood.

\subsection{Self-supervised loss}
Following \cite{batson2019noise2self}, we used a self-supervised loss with pixel masking along a grid. The loss is computed only on the masked pixels.
We obtained the best results with a replacement by a 3x3 gaussian filter with $\sigma=1$ with a weight equal to zero at the center position.
The drawback of maksing along a grid is that for a given receptive field, pixels are masked at fixed positions. If grid spacing is too low, then too many masked pixels are present in the receptive field and perturbs the performances, because the network learn to avoid using masked pixels for prediction. On the other hand, the larger the spacing, the less pixels are used for training, which reduces dramatically training efficiency.
In order to push the limits of this trade-off between learning efficiency and denoising quality, we introduced two modifications on the original approach: we used a random dynamic spacing between 3 and 5 pixels and we also dropped out randomly 10\% of the grid points. Both modifications allow to have relative positions of masked pixels that change randomly, and thus reduce considerably the grid spacing without drop of performances. The value of the loss was normalized by the number of points in the grid.

Fluorescence-microscopy datasets usually display a high signal frequency imbalance, where background is overrepresented. To cope with this issue, we introduced a weight map to give more relative importance to rare signal during training. Let $p(x)$ the observed probability of a value within the training dataset and G the masking grid for a given mini-batch, the associated weight is $\frac{1 - p(x)}{\sum_{G}(1 - p(x))}$

\subsection{Datasets}
We trained and evaluated our method on 6 publicly available datasets of microscopy images. In those datasets, ground truth is estimated by averaging several observations of the same field-of-view (FOV).

The 3 first datasets have been published along with the PN2V method\cite{krull2019probabilistic}, each is composed of several observations of a single FOV. \emph{Convallaria} dataset is refered to as \emph{PN2V-C}, \emph{Mouse skull nuclei} as \emph{PN2V-MN} and \emph{Mouse Actin} as \emph{PN2V-MA}. We used the same training and evaluation sets as the authors: for each sample type the whole dataset is used for training, and only a subset of the FOV is used for PSNR computation: $(Y,X)\in([0, 512], [0, 512])$ for PN2V-C, $([0, 512], [0, 256])$ for PN2V-MN and $([0, 1024], [0, 512])$ for PN2V-MA.

The 3 last datasets are the 3 channels of the W2S dataset\cite{zhou2020w2s} refered to as \emph{W2S-1}, \emph{W2S-2} and \emph{W2S-3}. We used the 16-bit raw images kindly provided by the authors. The dataset is composed of 120 FOV, the first 80 are used for training and the last 40 for evaluation. Following the authors, for each FOV, only the raw image of index 249 is used for training and for evaluation.

This dataset contains also alinged super-resolution images (referred to as SIM-GT) which were used to evaluate the deconvolution procedure. We downscaled the SIM-GT image using a 2x2 average pooling transform in order to have a pixel-wise correspondance with wide-field (WF) images. In order to have the SIM-GT and WF images in the same intensity range, for each FOV we scaled linearily the SIM-GT image in order to have the modal values and the 95\% percentile correspond with the WF ground truth image.

\subsection{Training}
Networks are trained using Adam optimizer with a learning rate of $10^{-4}$, decreased by $\frac{1}{2}$ on plateau of 15 epochs until $10^{-6}$. We trained networks for 300 steps of 100 epochs.
We obtained better and more reproductible results using the weights of the trained model at the last epoch instead of the weights of the model with the best validation loss, possibliy because the loss is a bad proxy for the denoising performances. For that reason, we did not use a validation step.
Each batch is composed of (overlapping) random tiles of 96x96 pixels for microscopy datasets. Batch size was set to 4 and tile overlap to a value that yielded in mini batches of 64 tiles. Tiles are augmented with random horizontal and/or vertical flip and/or a random rotation with an angle chosen within $(90, 180, 270)$.
[For natural images tile size was set to 128x128.]
\begin{itemize}
\item average flip
\item alternate procedure ?
\end{itemize}

\subsection{Evaluation}
For the 6 chosen datasets, images are encoded in 16-bit thus the actual data range of each ground truth image is used for PSNR computation.
When comparing images, the PSNR is not highly indicative of perceived similarity, in particular it do not reflect similarity of high frequency information such as textures and local contrasts\cite{wang2004image}, that denoising methods tend to reduce. It is thus essential to have other metrics that them into account.
To address this shortcoming, we used Structural similarity (SSIM) that take texture into account. It was computed as in \cite{wang2004image}.
In order to take into account local contrasts, we introduced a very simple metric: we computed PSNR of gradient magnitude images (GPSNR). This metric simply reflects whether an image is blurry or sharp, compared to a ground truth image.
GPSNR is computed by transforming both ground truth and sample images with gaussian gradient magnitude with $\sigma=1$, using the range of the gradient magnitude transform of the ground truth image.

\section{Results}
We compared our method to 2 baselines the current state-of-the-art blind self-supervised denoising method for bio-images, DecoNoising, as well as one of the most simple denoising method: convolution by a gaussian. For each dataset we chose the optimal standard deviation of the gaussian so that it maximizes the PSNR. We believe it makes a good reference, as it actually removed efficiently noise, but also local contrasts, so a good denoising method should perform better both in terms of PNSR and GPSNR/SSIM. We were suprised to notice that N2V do not permforms better than the optimal gaussian in terms of PSNR for 3 of the 6 datasets.


\section{Conclusion}



{\small
\bibliography{blind_denoising}
\bibliographystyle{icml2021}
}

\end{document}
