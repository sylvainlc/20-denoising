% This version of CVPR template is provided by Ming-Ming Cheng.
% Please leave an issue if you found a bug:
% https://github.com/MCG-NKU/CVPR_Template.

\documentclass[review]{cvpr}
%\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\confYear{CVPR 2021}
%\setcounter{page}{4321} % For final version only


\begin{document}

%%%%%%%%% TITLE
\title{Joint self-supervised blind denoising and noise estimation}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\maketitle


%%%%%%%%% ABSTRACT
\begin{abstract}
Self-supervised deep neural networks trained for blind denoising have recently emerged and outperformed supervised networks. Using the assumption that the signal has local correlation and that the noise components are independent, such networks are able to predict an estimate of the clean signal without clean training data. Therefore they are particularly relevant for biomedical image denoising where the noise process is difficult to model precisely and clean training data are usually unavailable. However they suffer from a low training efficiency and produce visual artifacts, which strongly limits the denoising performances.
In this work we describe a model that enables to estimate the clean signal prior and the noise distribution jointly with very few assumptions on the noise. This model is implemented through two neural networks trained simultaneously, in which careful architecture choices enable us to significantly improve the quality of denoising as well as providing an accurate noise distribution. Our framework also introduces significant improvements of the training efficiency while being simple to implement and lightweight in terms of parameters and operations.

Our method improves significantly the performance of current state-of-the-art self-supervised blind denoising networks both visually and quantitatively according to classical metrics, on six publicly available biomedical image datasets. We also show empirically on synthetic noisy data that we are able to capture the noise distribution efficiently. We also introduce a simple metric to estimate the sharpness of denoised images and show that our method produces sharper images than previous methods.
\end{abstract}

\section{Introduction}
\label{sec:introduction}
[D'abord un blabla sur l'importance du denoising. + particulier dans le contexte bio-image ]
Classical denoising methods, are model-driven methods in the sense that they rely on assumptions on the noise process or on the structure of the signal but are limited by the relevance of these assumptions.
Recently, efficient data-driven methods have emerged. Most of them are using pairs of noisy and clean measurements of the same signal in a supervised learning problem \cite{weigert2017content}. Lethinen et al. have demonstrated that it is possible to train an efficient denoising method using pairs of independent noisy measurements instead of a clean measurement of the same signal\cite{lehtinen2018noise2noise}. However clean measurements or pairs of independent noisy measurements of the siganl are often unavailable in practice.
Recent self-supervised methods overcome this limitation\cite{batson2019noise2self,krull2018noise2void}. The main idea of those methods is that a neural network learns to predict a clean pixel only using its surroundings. We refer to those methods as blind-denoising methods because they make no assumption on the nature of the corrupting process other than [0-mean + statistical independance].
The previously cited methods as well as their improved versions all display a high-frequency artefact refered to as \textit{checkerboard pattern} that limit their denoising capabilities.

The contribution of this work is two-fold:
\begin{itemize}
\item We solve the \textit{checkerboard pattern} limitation through modifications of the network architecture as well as optimization of the training procedure
\item We further improve the denoising performances by introducing a modelisation of the noise process that is learnt with a second neural network
\end{itemize}

\section{Related work}
\label{sec:related}
\begin{itemize}
\item Introduce notion of J-invariant function
\end{itemize}

Two types of self-supervised methods:
\paragraph{Masked training}
\begin{itemize}
\item Introduced by Noise2Noise\cite{krull2018noise2void} (N2N) and Noise2Self \cite{batson2019noise2self} (N2S).
\item describe self-supervised loss. only masked pixels are used for training, which represent a few percent per image.
\item masking strategies. explain the differences and flaws: N2N: no minimal distance between masked pixels + a chance that a pixel is replaced by itself/ N2S: grid replacement: mask also other pixels of the receptive field + replace by donut average.
\item PN2V\cite{krull2019probabilistic} and PPN2V\cite{prakash2020fully}: use a noise model, which slighlty improve performances without solving the checkerboard pattern.
\item deconoising:\cite{goncharova2020} add a gaussian conlolution after the NN output to simulate microscope PSF improves performances, however the deconvolved image (predicted image before the gaussian conlolution) has even stronger checkerboard pattern.
\item In a related work, Kobayashi et al. show that self-supervised netwoks can perform efficiel noise-tolerant deconvolution, on sythetic data \cite{kobayashi2020image}.
\item When noise processe displays local correlations that are known masking can be adapted to remove them \cite{broaddus2020removing}
\end{itemize}

\paragraph{J-invariant networks}
\begin{itemize}
\item explain that the previous architectures are not J-invariant (can be turned into J-invariant functions by masking pixels before predicting their denoised value).
\item Existing J-invariant architecture: \cite{laine2019high} using directional kernels. Introduction of a noise model $\rightarrow$ allows to improve performances on synthetic noise, but not suitable for real noise. Intuition that J-invariant functions can be improved by adding the information of the central pixel.
\item advantage over masked training : higher training efficiency : the whole image is used for training.
\item Other J-invariant architectures: \cite{lee2020noise2kernel}, strong limitation on network architecture: only one layer of convolution with center excluded followed by strided convolutions.
\item Structured noise cannot be removed.
\end{itemize}

\section{Model}
\label{sec:model}
The blind denoising framework introduced in this paper can be written as a regression problem where the observations conditionally on $\Omega_Y$ the observation is given by
$$
Y = \mu_{\theta_p}(\Omega_Y;\overline{\Omega_Y}) + \sigma_{\theta_n}(\mu_{\theta_p}(\Omega_Y;\overline{\Omega_Y}))\varepsilon\,,
$$
where $\varepsilon$ is a standard Gaussian random variable. The approach proposed in this paper is decomposed into two steps.
\begin{enumerate}
\item First, $\mu_{\theta_p}$ is obtained with the P-Net fed with $\Omega_y$ and $\overline \Omega_y$ at central position, $\sigma_{\theta_n}^2$ is  obtained with the N-Net fed with $\mu_{\theta_p}$.
\item After a training procedure to estimate the parameters of these networks, the estimator of the unknow pixel given the observations $(y,\Omega_y)$ is set to $\mu_{\theta_p}(\Omega_y,y)$ as an approximation of the posterior mean.
\end{enumerate}
The loss function is defined by approximating the loglikelihood of the observations using samples from the P-net:
$$
\ell_{\theta}: (Y_1,\ldots,Y_N) \mapsto \frac{1}{N}\sum_{i=1}^N \ell_{\theta}(Y_i|\Omega_{Y_i})\,,
$$
where
\begin{multline*}
\ell_{\theta}(Y_i|\Omega_{Y_i}) = \log(\sigma_{\theta_n}(\mu_{\theta_p}(\Omega_{Y_i};\overline{\Omega_{Y_i}}))^2) \\
+ \frac{(Y_i-\mu_{\theta_p}(\Omega_{Y_i};\overline{\Omega_{Y_i}}))^2}{\sigma_{\theta_n}(\mu_{\theta_p}(\Omega_{Y_i};\overline{\Omega_{Y_i}}))^2} \,.
\end{multline*}

\section{Implementation}
\subsection{P-Net}
The function $x\mapsto (\mu_p , \sigma_p)$ is implemented through a UNet \cite{ronneberger2015u} deep neural network, which has a fully convolutional architecture.
We made a few changes from the original version: we do not crop the image and use zero-padding instead, we use 2 levels of contractions/expansions with 64 filters, expansions is performed by an upsampling layer with nearest-neighbor approximation (followed by 2x2 convolution?), we added two layers of 1x1 convolution with 64 filters and ReLU activation at the end of the network, there is no activation function at the output layer.

% cette section ne devrait-elle pas aller ailleurs ?
This implies that at a given coordinate $(l,m)$, $(\mu_p^{(l,m)}, \sigma_p^{(l,m)})$ depend on $x^{(l,m)}$ and its neighborhood.
While the central pixel $x^{(l,m)}$ is masked during training (its value is replaced with a deterministic function of the neighboring pixels), the convolutional architecture still uses the replaced value and learns the parameters of the convolution associated with this central value.

\subsection{N-Net}
the function $x\mapsto \sigma_n$ describing the noise distribution is implemented through a fully convolutional network only composed of six 1x1 convolutions of 64 filters followed a non-linear activation layer (alternatively tanh and relu).
We found that such networks could be trained in a supervised way with a clean image x as input and the following cost function, where y is a corrupted version of x:
$$
y\mapsto \varphi_{0,\sigma_n^2}(y)
$$
An essential aspect of the architecture is that the network contain no spatial convolution (only 1x1 convolutions), otherwise the noise distribution is not well described by the network. This is consistent with our model, in which the noise is independent of the neighborhood.

\subsection{Self-supervised loss}
Following \cite{batson2019noise2self}, we used a self-supervised loss with pixel masking along a grid. The loss is computed only on the masked pixels.
We obtained the best results with a replacement by a 3x3 gaussian filter with $\sigma=1$ with a weight equal to zero at the center position.
The drawback of maksing along a grid is that for a given receptive field, pixels are masked at fixed positions. If grid spacing is too low, then too many masked pixels are present in the receptive field and perturbs the performances, because the network learn to avoid using masked pixels for prediction. On the other hand, the larger the spacing, the less pixels are used for training, which reduces dramatically training efficiency.
In order to push the limits of this trade-off between learning efficiency and denoising quality, we introduced two modifications on the original approach: we used a random dynamic spacing between 3 and 5 pixels and we also dropped out randomly 10\% of the grid points. Both modifications allow to have relative positions of masked pixels that change randomly, and thus reduce considerably the grid spacing without drop of performances. The value of the loss was normalized by the number of points in the grid.

Fluorescence-microscopy dataset usually display a high signal frequency imbalance, where background is overrepresented. To cope with this issue, we introduced a weight map to give more relative importance to rare signal during training. Let $p(x)$ the observed probability of a value within the training dataset and G the masking grid for a given mini-batch, the associated weight is $\frac{1 - p(x)}{\sum_{G}(1 - p(x))}$

\section{Training and evaluation}
\subsection{Datasets}
We trained and evaluated our method on 6 publicly available datasets of microscopy images. In those datasets, ground truth is estimated by averaging several observations of the same field-of-view (FOV).

The 3 first datasets have been published along with the PN2V method\cite{krull2019probabilistic}, each is composed of several observations of a single FOV. \emph{Convallaria} dataset is refered to as \emph{PN2V-C}, \emph{Mouse skull nuclei} as \emph{PN2V-MN} and \emph{Mouse Actin} as \emph{PN2V-MA}. We used the same training and evaluation sets as the authors: for each sample type the whole dataset is used for training, and only a subset of the FOV is used for PSNR computation: $(Y,X)\in([0, 512], [0, 512])$ for PN2V-C, $([0, 512], [0, 256])$ for PN2V-MN and $([0, 1024], [0, 512])$ for PN2V-MA.

The 3 last datasets are the 3 channels of the W2S dataset\cite{zhou2020w2s} refered to as \emph{W2S-1}, \emph{W2S-2} and \emph{W2S-3}. We used the 16-bit raw images kindly provided by the authors. The dataset is composed of 120 FOV, the first 80 are used for training and the last 40 for evaluation. Following the authors, for each FOV, only the raw image of index 249 is used for training and for evaluation.

This dataset contains also alinged super-resolution images (referred to as SIM-GT) which were used to evaluate the deconvolution procedure. We downscaled the SIM-GT image using average pooling in order to have a pixel-to-pixel correspondance. In order to have the SIM-GT and wide-field (WF) images in the same intensity range, for each FOV, we scaled linearily the SIM-GT image in order to have the modal values and the 95\% percentile correspond between the WF ground truth and the SIM-GT images.

\subsection{Training}
We used Adam optimizer with start learning rate $10^{-4}$, decreased by $\frac{1}{2}$ on plateau of 15 epochs. We trained networks for 300 steps of 100 epochs. We obtained better and more reproductible results by using the weights of the trained model at the last epoch instead of the weights of the model with the best validation loss, possibliy because the loss is a bad proxy for the denoising performances. Thus we removed the validation step and used the weights of trained model at the end of training.
\begin{itemize}
\item average flip
\item alternate procedure ? 
\end{itemize}
\subsection{Evaluation}
For the 6 chosen datasets, images are encoded in 16-bit thus the actual data range of each ground truth image is used for PSNR computation.
Structural similarity (SSIM) was computed as in \cite{wang2004image}.
To compute PSNR of gradient images (GPSNR), we transformed both ground truth and sample images with gaussian gradient magnitude with $\sigma=1$, and computed the PSNR of the two images using the range of the gradient magnitude transform of the ground truth image.

To further

\section{Conclusion}



{\small
\bibliographystyle{ieee_fullname}
\bibliography{blind_denoising}
}

\clearpage
\newpage

\section{Thoughts for future research}
\subsection{Training based on Variational Auto-Encoders}
The aim of this paper is to estimate $p_{\theta}(x|Y,\Omega_Y)$ to obtain samples from the posterior distribution of the signal given the noisy observations obtained in its neighborhood. An appealing approch to appraoximate $p_{\theta}(x|Y,\Omega_Y)$  is to introduce a parametric family of candidate approximations $q_{\phi}(x|Y,\Omega_Y)$. In this case,
\begin{align*}
\log p_{\theta}(y|\Omega_y) &\\
&\hspace{-1.3cm}= \mathbb{E}_{q_{\phi}(\cdot|y,\Omega_y)}[\log p_{\theta}(y|\Omega_y)]\,,\\
&\hspace{-1.3cm}= \mathbb{E}_{q_{\phi}(\cdot|y,\Omega_y)}\left[\log \frac{p_\theta(X,y|\Omega_y)}{q_{\phi}(\cdot|y,\Omega_y)}\frac{q_{\phi}(\cdot|y,\Omega_y)}{p_\theta(X|y,\Omega_y)}\middle |y,\Omega_y\right]\,,\\
&\hspace{-1.3cm}= \mathcal{L}_{\theta,\phi}(y,\Omega_y) + \mathrm{KL}\left(q_{\phi}(\cdot|y,\Omega_y)\| p_\theta(\cdot|y,\Omega_y)\right)\,,
\end{align*}
where $ \mathrm{KL}$ is the Kullback-Leibler divergence and where $\mathcal{L}_{\theta,\phi}(y,\Omega_y)$ is the evidence lower bound (ELBO):
$$
 \mathcal{L}_{\theta,\phi}(y,\Omega_y)  = \mathbb{E}_{q_{\phi}(\cdot|y,\Omega_y)}\left[\log \frac{p_\theta(X,y|\Omega_y)}{q_{\phi}(X|y,\Omega_y)}\middle |y,\Omega_y\right]\,.
$$
Therefore, $ \mathcal{L}_{\theta,\phi}(y,\Omega_y) \leqslant \log p_{\theta}(y|\Omega_y)$ and $ \mathcal{L}_{\theta,\phi}(y,\Omega_y)$ is usually used as a surrogate for $ \log p_{\theta}(y|\Omega_y)$. The aim of auto-encoders approaches is then to obtain
$$
(\theta^*,\phi^*) \in \mathrm{argmax} \; \mathcal{L}_{\theta,\phi}(y,\Omega_y)\,.
$$
Consider the following model and variational family.
\begin{itemize}
\item $p_\theta(x|\Omega_y)$: Gaussian prior, P-Net with $\overline \Omega_y$ at central position.
\item $p_\theta(y|x,\Omega_y)$: N-net.
\item $q_{\phi}(\cdot|y,\Omega_y)$: Gaussian posterior, P-Net with $y$ at central position.
\end{itemize}
The loss function (to be minimized) can also be written
\begin{multline*}
 \mathcal{L}_{\theta,\phi}(y,\Omega_y)  =   -\mathbb{E}_{q_{\phi}(\cdot|y,\Omega_y)}\left[\log p_\theta(y|X,\Omega_y)\middle |y,\Omega_y\right] \\ + \mathrm{KL}\left(q_{\phi}(\cdot|y,\Omega_y)\| p_\theta(\cdot|\Omega_y)\right)\,.
\end{multline*}
For one sample $(y_i,\Omega_i)$, using the explicit computation of the  Kullback-Leibler divergence between Gaussian distributions, this yields:
\begin{multline*}
2\widehat{\mathcal{L}}_{\theta,\phi}(y_i,\Omega_i)  =   \log(\sigma^2_{\theta_n}(X_i,\Omega_i)) + \frac{(Y_i-X_i)^2}{\sigma^2_{\theta_n}(X_i,\Omega_i)} \\ + \log\frac{\sigma^2_{\theta_p}(\Omega_i)}{\sigma^2_{\phi}(Y_i,\Omega_i)} + \frac{\sigma^2_{\phi}(Y_i,\Omega_i) + \left(\mu_{\theta_p}(\Omega_i) - \mu_\phi(Y_i,\Omega_i)\right)^2}{\sigma^2_{\theta_p}(\Omega_i)}\,,
\end{multline*}
where $X_i$ is a sample from $q_\phi(\cdot|y_i,\Omega_i)$ i.e.
$$
X_i = \mu_\phi(Y_i,\Omega_i) + \sigma_\phi(Y_i,\Omega_i)\varepsilon_i
$$
with $\varepsilon_i$ a standard Gaussian random variable.

The importance weigthed bound is a variational lower bound of the loglikelihood based on importance sampling which can be applied to variational autoencoders. This bound is given by:
\begin{multline*}
 \mathcal{L}^{\mathsf{IWAE}}_{\theta,\phi}(y,\Omega_y)  \\
= \mathbb{E}_{q^{\otimes M}_{\phi}(\cdot|y,\Omega_y)}\left[\log \left(\frac{1}{M}\sum_{m=1}^M\frac{p_\theta(X_m,y|\Omega_y)}{q_{\phi}(X_m|y,\Omega_y)}\right)\middle |y,\Omega_y\right]\,,
\end{multline*}
where $\mathbb{E}_{q^{\otimes M}_{\phi}(\cdot|y,\Omega_y)}$ means that the $(X_m)_{1\leqslant m \leqslant M}$ are independent with distribution $q_{\phi}(\cdot|y,\Omega_y)$. This loss is approximated with $M$ samples from $q_{\phi}(\cdot|y,\Omega_y)$:
$$
 \widehat{\mathcal{L}}^{\mathsf{IWAE}}_{\theta,\phi}(y,\Omega_y)
= \log \left(\frac{1}{M}\sum_{m=1}^M\frac{p_\theta(X_m,y|\Omega_y)}{q_{\phi}(X_m|y,\Omega_y)}\right)\,.
$$
Note that the gradient (using the reparametrization trick for $\phi$)  can be written:
$$
\sum_{m=1}^M \frac{\omega_{\theta,\phi}(X_m)}{\sum_{\ell=1}^M\omega_{\theta,\phi}(X_\ell)} \nabla\log \omega_{\theta,\phi}(X_m)\,,
$$
where
$$
\omega_{\theta,\phi}(X_m) = \frac{p_\theta(X_m,y|\Omega_y)}{q_{\phi}(X_m|y,\Omega_y)}\,.
$$

Using that:
$$
p_\theta(X_m,y|\Omega_y) = p_\theta(X_m|\Omega_y) p_\theta(y|X_m)
$$
yields:
\begin{multline*}
2\log(\omega_{\theta,\phi}(X_m)) = \\
- \frac{( \mu_{\theta_p}(\Omega_i) - X_m )^2}{\sigma^2_{\theta_p}(\Omega_i)} - \log(\sigma^2_{\theta_p}(\Omega_i)) \\
- \frac{( Y_i - X_m )^2}{\sigma^2_{\theta_n}(X_i,\Omega_i)} - \log(\sigma^2_{\theta_n}(X_i,\Omega_i)) \\
+ \frac{( \mu_\phi(Y_i,\Omega_i) - X_m )^2}{\sigma^2_{\phi}(Y_i,\Omega_i)} + \log(\sigma^2_{\phi}(Y_i,\Omega_i))
\end{multline*}

\end{document}
