% This version of CVPR template is provided by Ming-Ming Cheng.
% Please leave an issue if you found a bug:
% https://github.com/MCG-NKU/CVPR_Template.

\documentclass[review]{cvpr}
%\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\confYear{CVPR 2021}
%\setcounter{page}{4321} % For final version only


\begin{document}

%%%%%%%%% TITLE
\title{Joint self-supervised blind denoising and noise estimation}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\maketitle


%%%%%%%%% ABSTRACT
\begin{abstract}
Self-supervised deep neural networks trained for blind denoising have recently emerged and outperformed supervised networks. Using the assumption that the signal has local correlation and that the noise components are independent, such networks are able to predict an estimate of the clean signal without clean training data. Therefore they are particularly relevant for biomedical image denoising where the noise process is difficult to model precisely and clean training data are usually unavailable. However they suffer from a low training efficiency and produce visual artifacts, which strongly limits the denoising performances.
In this work we describe a model that enables to estimate the clean signal prior and the noise distribution jointly with very few assumptions on the noise. This model is implemented through two neural networks trained simultaneously, in which careful architecture choices enable us to significantly improve the quality of denoising as well as providing an accurate noise distribution. Our framework also introduces significant improvements of the training efficiency while being simple to implement and lightweight in terms of parameters and operations.

Our method improves significantly the performance of current state-of-the-art self-supervised blind denoising networks both visually and quantitatively according to classical metrics, on six publicly available biomedical image datasets. We also show empirically on synthetic noisy data that we are able to capture the noise distribution efficiently. We also introduce a simple metric to estimate the sharpness of denoised images and show that our method produces sharper images than previous methods.
\end{abstract}

\section{Introduction}
\label{sec:introduction}
[D'abord un blabla sur l'importance du denoising. + particulier dans le contexte bio-image ]
Classical denoising methods, are model-driven methods in the sense that they rely on assumptions on the noise process or on the structure of the signal but are limited by the relevance of these assumptions.
Recently, efficient data-driven methods have emerged. Most of them are using pairs of noisy and clean measurements of the same signal in a supervised learning problem \cite{weigert2017content}. Lethinen et al. have demonstrated that it is possible to train an efficient denoising method using pairs of independent noisy measurements instead of a clean measurement of the same signal\cite{lehtinen2018noise2noise}. However clean measurements or pairs of independent noisy measurements of the siganl are often unavailable in practice.
Recent self-supervised methods overcome this limitation\cite{batson2019noise2self,krull2018noise2void}. The main idea of those methods is that a neural network learns to predict a clean pixel only using its surroundings. We refer to those methods as blind-denoising methods because they make no assumption on the nature of the corrupting process other than [0-mean + statistical independance].
The previously cited methods as well as their improved versions all display a high-frequency artefact refered to as \textit{checkerboard pattern} that limit their denoising capabilities.

The contribution of this work is two-fold:
\begin{itemize}
\item We solve the \textit{checkerboard pattern} limitation through modifications of the network architecture as well as optimization of the training procedure
\item We further improve the denoising performances by introducing a modelisation of the noise process that is learnt with a second neural network
\end{itemize}

\section{Related work}
\label{sec:related}
\begin{itemize}
\item Introduce notion of J-invariant function
\end{itemize}

Two types of self-supervised methods:
\paragraph{Masked training}
\begin{itemize}
\item Introduced by Noise2Noise\cite{krull2018noise2void} (N2N) and Noise2Self \cite{batson2019noise2self} (N2S).
\item describe self-supervised loss. only masked pixels are used for training, which represent a few percent per image.
\item masking strategies. explain the differences and flaws: N2N: no minimal distance between masked pixels + a chance that a pixel is replaced by itself/ N2S: grid replacement: mask also other pixels of the receptive field + replace by donut average.
\item PN2V\cite{krull2019probabilistic} and PPN2V\cite{prakash2020fully}: use a noise model, which slighlty improve performances without solving the checkerboard pattern.
\item deconoising:\cite{goncharova2020} add a gaussian conlolution after the NN output to simulate microscope PSF improves performances, however the deconvolved image (predicted image before the gaussian conlolution) has even stronger checkerboard pattern.
\item In a related work, Kobayashi et al. show that self-supervised netwoks can perform efficiel noise-tolerant deconvolution, on sythetic data \cite{kobayashi2020image}.
\item When noise processe displays local correlations that are known masking can be adapted to remove them \cite{broaddus2020removing}
\end{itemize}

\paragraph{J-invariant networks}
\begin{itemize}
\item explain that the previous architectures are not J-invariant (can be turned into J-invariant functions by masking pixels before predicting their denoised value).
\item Existing J-invariant architecture: \cite{laine2019high} using directional kernels. Introduction of a noise model $\rightarrow$ allows to improve performances on synthetic noise, but not suitable for real noise. Intuition that J-invariant functions can be improved by adding the information of the central pixel.
\item advantage over masked training : higher training efficiency : the whole image is used for training.
\item Other J-invariant architectures: \cite{lee2020noise2kernel}, strong limitation on network architecture: only one layer of convolution with center excluded followed by strided convolutions.
\item Structured noise cannot be removed.
\end{itemize}

\section{Model}
\label{sec:model}
The blind denoising framework introduced in this paper can be written as a regression problem where the observations conditionally on $\Omega_Y$ the observation is given by
$$
Y = \mu_{\theta_p}(\Omega_Y;\overline{\Omega_Y}) + \sigma_{\theta_n}(\mu_{\theta_p}(\Omega_Y;\overline{\Omega_Y}))\varepsilon\,,
$$
where $\varepsilon$ is a standard Gaussian random variable. The approach proposed in this paper is decomposed into two steps.
\begin{enumerate}
\item First, $\mu_{\theta_p}$ is obtained with the P-Net fed with $\Omega_y$ and $\overline \Omega_y$ at central position, $\sigma_{\theta_n}^2$ is  obtained with the N-Net fed with $\mu_{\theta_p}$.
\item After a training procedure to estimate the parameters of these networks, the estimator of the unknow pixel given the observations $(y,\Omega_y)$ is set to $\mu_{\theta_p}(\Omega_y,y)$ as an approximation of the posterior mean.
\end{enumerate}
The loss function is defined by approximating the loglikelihood of the observations using samples from the P-net:
$$
\ell_{\theta}: (Y_1,\ldots,Y_N) \mapsto \frac{1}{N}\sum_{i=1}^N \ell_{\theta}(Y_i|\Omega_{Y_i})\,,
$$
where
\begin{multline*}
\ell_{\theta}(Y_i|\Omega_{Y_i}) = \log(\sigma_{\theta_n}(\mu_{\theta_p}(\Omega_{Y_i};\overline{\Omega_{Y_i}}))^2) \\
+ \frac{(Y_i-\mu_{\theta_p}(\Omega_{Y_i};\overline{\Omega_{Y_i}}))^2}{\sigma_{\theta_n}(\mu_{\theta_p}(\Omega_{Y_i};\overline{\Omega_{Y_i}}))^2} \,.
\end{multline*}

\section{Implementation}
\subsection{P-Net}
The function $x\mapsto (\mu_p , \sigma_p)$ is implemented through a UNet \cite{ronneberger2015u} deep neural network, which has a fully convolutional architecture.
The network consists of a contracting path and an expansive path, which gives it the u-shaped architecture. The contracting path is a typical convolutional network that consists of repeated application of convolutions, each followed by a rectified linear unit (ReLU) and a max pooling operation. During the contraction, the spatial information is reduced while feature information is increased. The expansive pathway combines the feature and spatial information through a sequence of up-convolutions and concatenations with high-resolution features from the contracting path.
We use 2 levels of contractions with 64 filters.
This implies that at a given coordinate $(l,m)$, $(\mu_p^{(l,m)}, \sigma_p^{(l,m)})$ depend on $x^{(l,m)}$ and its neighborhood.

While the central pixel $x^{(l,m)}$ is masked during training (its value is replaced with a deterministic function of the neighboring pixels), the convolutional architecture still uses the replaced value and learns the parameters of the convolution associated with this central value.

\subsection{N-Net}
the function $x\mapsto \sigma_n$ describing the noise distribution is implemented through a fully convolutional network only composed of 1x1 convolutions followed a non-linear activation layer (alternatively tanh and relu).
We found that such networks could be trained in a supervised way with a clean image x as input and the following cost function, where y is a corrupted version of x:
$$
y\mapsto \varphi_{0,\sigma_n^2}(y)
$$
An essential aspect of the architecture is that the network contain no spatial convolution (only 1x1 convolutions), otherwise the noise distribution is not well described by the network. This is consistent with our model, in which the noise is independent of the neighborhood.

\subsection{Self-supervised loss}
Following \cite{batson2019noise2self}, we used a self-supervised loss with pixel masking along a grid. The loss is computed only on the masked pixels.
We obtained the best results with a replacement by a 3x3 gaussian filter excluding the center pixel.
We also optimized grid spacing as there is a trade-off between learning efficiency and denoising quality: the smaller the spacing, the more pixels are used for training, but the more masked pixel are present in the receptive field which perturbs the denoising. We obtained best results using a random dynamic spacing between 3 and 5 pixels and we also remove randomly 10\% of the grid. The value of the loss was normalized by the number of pixel in the grid.


\section{Training and evaluation datasets}
We evaluated our method on 3 publicly available datasets of microscopy images. To estimate a ground truth image, several observations of the same field-of-view (FOV) are acquired and averaged.
The first dataset is taken from FMD dataset (mice cells) published by. It is composed of 20 different fields of view (FOV), each one containing 50 independent observations of the same sample. We refer to this dataset as \emph{FMD-Confocal-Mice}. We used the same training and evaluation procedure as the authors, i.e. evaluation on the 19th FOV and training/validation on the 19 other FOVs. We only used raw images. Note that the PSNR of raw images from different FOVs vary in a range of ~2dB, thus it is essential to use the same FOVs to compare absolute PSNR values between different works. Image are encoded in 8-bit and 255 is used as data range for PSNR computation.

The second dataset has been published along with the PPN2V method, and is composed of 3 different samples types and only one FOV per sample type. We refer to each sample type of this dataset as \emph{PPN2V-Convallaria}, \emph{PPN2V-Mouse-Actin} and \emph{PPN2V-Mouse-skull-nuclei}. We used the same training and evaluation procedure as the authors. For each sample type the whole dataset is used for training, and only a part of the FOV is used for PSNR computation. Images are encoded in 16-bit and actual data range of the ground truth is used for PSNR computation.

The third dataset is published W2S. We used the 16-bit raw images kindly provided by the authors. The dataset is composed of 120 FOV, the first 80 are used for training/validation and the last 40 for evaluation. Following the authors, for each FOV, only the raw image of index 249 is used for training/validation and for evaluation. As Images are encoded in 16-bit, actual data range of the ground truth is used for PSNR computation.
This dataset contains also super-resolution images (referred to as SIM-GT) which were used to evaluate the deconvolution procedure. In order to have the SIM-GT and wide-field (WF) images in the same intensity range, for each FOV, the average of all WF images was computed (referred to as WF-GT), and intensity values of SIM-GT were scaled linearly in order to minimize
$( \textrm{SIM-GT} - \textrm{WF-GT})^2$

\section{Conclusion}



{\small
\bibliographystyle{ieee_fullname}
\bibliography{blind_denoising}
}

\clearpage
\newpage

\section{Thoughts for future research}
\subsection{Training based on Variational Auto-Encoders}
The aim of this paper is to estimate $p_{\theta}(x|Y,\Omega_Y)$ to obtain samples from the posterior distribution of the signal given the noisy observations obtained in its neighborhood. An appealing approch to appraoximate $p_{\theta}(x|Y,\Omega_Y)$  is to introduce a parametric family of candidate approximations $q_{\phi}(x|Y,\Omega_Y)$. In this case,
\begin{align*}
\log p_{\theta}(y|\Omega_y) &\\
&\hspace{-1.3cm}= \mathbb{E}_{q_{\phi}(\cdot|y,\Omega_y)}[\log p_{\theta}(y|\Omega_y)]\,,\\
&\hspace{-1.3cm}= \mathbb{E}_{q_{\phi}(\cdot|y,\Omega_y)}\left[\log \frac{p_\theta(X,y|\Omega_y)}{q_{\phi}(\cdot|y,\Omega_y)}\frac{q_{\phi}(\cdot|y,\Omega_y)}{p_\theta(X|y,\Omega_y)}\middle |y,\Omega_y\right]\,,\\
&\hspace{-1.3cm}= \mathcal{L}_{\theta,\phi}(y,\Omega_y) + \mathrm{KL}\left(q_{\phi}(\cdot|y,\Omega_y)\| p_\theta(\cdot|y,\Omega_y)\right)\,,
\end{align*}
where $ \mathrm{KL}$ is the Kullback-Leibler divergence and where $\mathcal{L}_{\theta,\phi}(y,\Omega_y)$ is the evidence lower bound (ELBO):
$$
 \mathcal{L}_{\theta,\phi}(y,\Omega_y)  = \mathbb{E}_{q_{\phi}(\cdot|y,\Omega_y)}\left[\log \frac{p_\theta(X,y|\Omega_y)}{q_{\phi}(X|y,\Omega_y)}\middle |y,\Omega_y\right]\,.
$$
Therefore, $ \mathcal{L}_{\theta,\phi}(y,\Omega_y) \leqslant \log p_{\theta}(y|\Omega_y)$ and $ \mathcal{L}_{\theta,\phi}(y,\Omega_y)$ is usually used as a surrogate for $ \log p_{\theta}(y|\Omega_y)$. The aim of auto-encoders approaches is then to obtain
$$
(\theta^*,\phi^*) \in \mathrm{argmax} \; \mathcal{L}_{\theta,\phi}(y,\Omega_y)\,.
$$
Consider the following model and variational family.
\begin{itemize}
\item $p_\theta(x|\Omega_y)$: Gaussian prior, P-Net with $\overline \Omega_y$ at central position.
\item $p_\theta(y|x,\Omega_y)$: N-net.
\item $q_{\phi}(\cdot|y,\Omega_y)$: Gaussian posterior, P-Net with $y$ at central position.
\end{itemize}
The loss function (to be minimized) can also be written
\begin{multline*}
 \mathcal{L}_{\theta,\phi}(y,\Omega_y)  =   -\mathbb{E}_{q_{\phi}(\cdot|y,\Omega_y)}\left[\log p_\theta(y|X,\Omega_y)\middle |y,\Omega_y\right] \\ + \mathrm{KL}\left(q_{\phi}(\cdot|y,\Omega_y)\| p_\theta(\cdot|\Omega_y)\right)\,.
\end{multline*}
For one sample $(y_i,\Omega_i)$, using the explicit computation of the  Kullback-Leibler divergence between Gaussian distributions, this yields:
\begin{multline*}
2\widehat{\mathcal{L}}_{\theta,\phi}(y_i,\Omega_i)  =   \log(\sigma^2_{\theta_n}(X_i,\Omega_i)) + \frac{(Y_i-X_i)^2}{\sigma^2_{\theta_n}(X_i,\Omega_i)} \\ + \log\frac{\sigma^2_{\theta_p}(\Omega_i)}{\sigma^2_{\phi}(Y_i,\Omega_i)} + \frac{\sigma^2_{\phi}(Y_i,\Omega_i) + \left(\mu_{\theta_p}(\Omega_i) - \mu_\phi(Y_i,\Omega_i)\right)^2}{\sigma^2_{\theta_p}(\Omega_i)}\,,
\end{multline*}
where $X_i$ is a sample from $q_\phi(\cdot|y_i,\Omega_i)$ i.e.
$$
X_i = \mu_\phi(Y_i,\Omega_i) + \sigma_\phi(Y_i,\Omega_i)\varepsilon_i
$$
with $\varepsilon_i$ a standard Gaussian random variable.

The importance weigthed bound is a variational lower bound of the loglikelihood based on importance sampling which can be applied to variational autoencoders. This bound is given by:
\begin{multline*}
 \mathcal{L}^{\mathsf{IWAE}}_{\theta,\phi}(y,\Omega_y)  \\
= \mathbb{E}_{q^{\otimes M}_{\phi}(\cdot|y,\Omega_y)}\left[\log \left(\frac{1}{M}\sum_{m=1}^M\frac{p_\theta(X_m,y|\Omega_y)}{q_{\phi}(X_m|y,\Omega_y)}\right)\middle |y,\Omega_y\right]\,,
\end{multline*}
where $\mathbb{E}_{q^{\otimes M}_{\phi}(\cdot|y,\Omega_y)}$ means that the $(X_m)_{1\leqslant m \leqslant M}$ are independent with distribution $q_{\phi}(\cdot|y,\Omega_y)$. This loss is approximated with $M$ samples from $q_{\phi}(\cdot|y,\Omega_y)$:
$$
 \widehat{\mathcal{L}}^{\mathsf{IWAE}}_{\theta,\phi}(y,\Omega_y)
= \log \left(\frac{1}{M}\sum_{m=1}^M\frac{p_\theta(X_m,y|\Omega_y)}{q_{\phi}(X_m|y,\Omega_y)}\right)\,.
$$
Note that the gradient (using the reparametrization trick for $\phi$)  can be written:
$$
\sum_{m=1}^M \frac{\omega_{\theta,\phi}(X_m)}{\sum_{\ell=1}^M\omega_{\theta,\phi}(X_\ell)} \nabla\log \omega_{\theta,\phi}(X_m)\,,
$$
where
$$
\omega_{\theta,\phi}(X_m) = \frac{p_\theta(X_m,y|\Omega_y)}{q_{\phi}(X_m|y,\Omega_y)}\,.
$$

Using that:
$$
p_\theta(X_m,y|\Omega_y) = p_\theta(X_m|\Omega_y) p_\theta(y|X_m)
$$
yields:
\begin{multline*}
2\log(\omega_{\theta,\phi}(X_m)) = \\
- \frac{( \mu_{\theta_p}(\Omega_i) - X_m )^2}{\sigma^2_{\theta_p}(\Omega_i)} - \log(\sigma^2_{\theta_p}(\Omega_i)) \\
- \frac{( Y_i - X_m )^2}{\sigma^2_{\theta_n}(X_i,\Omega_i)} - \log(\sigma^2_{\theta_n}(X_i,\Omega_i)) \\
+ \frac{( \mu_\phi(Y_i,\Omega_i) - X_m )^2}{\sigma^2_{\phi}(Y_i,\Omega_i)} + \log(\sigma^2_{\phi}(Y_i,\Omega_i))
\end{multline*}

\end{document}
